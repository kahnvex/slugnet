
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Slugnet &#8212; Slugnet 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/caption.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Layers" href="layers.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="slugnet">
<h1>Slugnet<a class="headerlink" href="#slugnet" title="Permalink to this headline">¶</a></h1>
<p>Slugnet is a modest expiremental neural networks library intended to solidify
the author’s understanding of deep learning.</p>
<p>The goal of this library is to mathematically document all relevant components
of a working neural networks library. This includes models, layers, optimizers,
activation functions, loss functions, forward propogation, backward
propogation, and more.</p>
<p>The mathematical documentation assumes basic understanding of discriminative
machine learning techniques, linear algebra, and calculus.</p>
<p>Before looking at any code, the following section will introduce the notation
styles this library will follow as well as give a brief mathematical
introduction to neural networks. In general, a neural network tries to
approximate some function <img class="math" src="_images/math/d0912de3a26537fc2fe761713c2e9d284607ca0e.svg" alt="f^*"/>, where <img class="math" src="_images/math/09467fcc0438cd7c1f8cf8fe693c7b558228dd97.svg" alt="\bm{y} = f^*(\bm{x})"/>. The
neural network implements a function <img class="math" src="_images/math/9adb2712ff7597296f847ca0d2f6f5859530d154.svg" alt="\hat{\bm{y}} = f(\bm{x})"/>, where
<img class="math" src="_images/math/54262162b9b81471ce8b512e5296071cfa65febf.svg" alt="\hat{\bm{y}}"/> represents the prediction made by the network, and
<img class="math" src="_images/math/b347786cc6323c42f81386c840043844c270dcad.svg" alt="f"/> represents the model. We say a neural network is fully connected if
each node in every layer is connected to every node in the adjacent layer. For
now, we will only consider fully connected neural networks.</p>
<div class="section" id="feedforward-mode">
<h2>Feedforward Mode<a class="headerlink" href="#feedforward-mode" title="Permalink to this headline">¶</a></h2>
<p>When making predictions, a neural network is said to be operating in
feedforward mode. For now, we will inspect how neural networks operate in
this mode.</p>
<div class="figure">
<p><img src="_images/tikz-adc8470529f14c217692ecb8a1fe29ff68ee34e9.png" alt="\tikzset{%
   brace/.style = { decorate, decoration={brace, amplitude=5pt} }
}

\draw [brace] (0.5,7)  -- (1.5,7) node[yshift=0.5cm, xshift=-0.5cm] {Input};
\draw [brace] (3.5,9)  -- (7.5,9) node[yshift=0.5cm, xshift=-1.9cm] {Hidden Layers};
\draw [brace] (9.5,6)  -- (10.5,6) node[yshift=0.5cm, xshift=-0.5cm] {Output};

\foreach \x/\n in {2/3, 4/2, 6/1}
   \draw(1,\x) circle(0.5cm)
   node {$x_{\n}$};

\foreach \x/\n in {0/5, 2/4, 4/3, 6/2, 8/1}
   \draw[fill=gray!30](4, \x) circle(0.5cm)
   node {$h_{\n}^{(1)}$};

\foreach \x/\n in {1/4, 3/3, 5/2, 7/1}
   \draw[fill=gray!30](7, \x) circle(0.5cm)
   node {$h_{\n}^{(2)}$};

\foreach \x/\n in {3/2, 5/1}
   \draw (10, \x) circle(0.5cm) node {$\hat{y}_{\n}$};

\foreach \x in {2,4,6}
   \foreach \y in {0, 2, 4, 6, 8}
      \draw[-{&gt;[scale=4]}, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](1,\x) -- (4,\y);

\foreach \x in {0,2,4,6,8}
   \foreach \y in {1,3,5,7}
      \draw[-&gt;, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](4,\x) -- (7,\y);

\foreach \x in {1,3,5,7}
   \foreach \y in {3,5}
      \draw[-&gt;, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](7,\x) -- (10,\y);

:libs: arrows,calc,positioning,shadows.blur,decorations.pathreplacing,arrows.meta" /></p>
</div><p class="caption"><strong>Figure 1:</strong> A three layer, fully connected neural network. The first layer
has five hidden units. The superscript number in parenthesis indicates the
layer of the unit. The index in subscript represents the unit’s index. For
example <img class="math" src="_images/math/0f553caa56681b28d6d9b70cee3ed32224fe45ef.svg" alt="h_3^{(4)}"/> represents the third unit of the forth layer.</p>
<p>We can write the network in figure 1 as
<img class="math" src="_images/math/ee6437cad19fd2574123720f0f0a858ba2ef7444.svg" alt="\bm{\hat{y}} = f(\bm{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\bm{x})))"/>. Each layer
<img class="math" src="_images/math/ffcc44595749ed9dfa62ba18fd1dff0dadb9ba62.svg" alt="f^{(i)}"/> is composed of the layer that came before it,
<img class="math" src="_images/math/091d43dc11acff048a12f403d5124d66d52c0337.svg" alt="f^{(i - 1)}"/>, the first layer <img class="math" src="_images/math/32e91852ddd1dd681567025ee8ff3b2c61ecccb6.svg" alt="f^{(1)}"/> takes the input
<img class="math" src="_images/math/9cb903938e2965877b3c1e0513f2545947e6ed25.svg" alt="\bm{x}"/>. Variables that are lowercase and bold represent vectors, and
variables that are capitalized and bold represent matrices. Additionally, we may
represent the network with the shorthand diagram below.</p>
<div class="figure">
<p><img src="_images/tikz-d9bccb73d8ea3ba26b4c2bec9f5308f8f54a1ad8.png" alt="\tikzset{%
   brace/.style = { decorate, decoration={brace, amplitude=5pt} }
}

\draw [brace] (0.5,2)  -- (1.5,2) node[yshift=0.5cm, xshift=-0.5cm] {Input};
\draw [brace] (3.5,2)  -- (7.5,2) node[yshift=0.5cm, xshift=-1.9cm] {Hidden Layers};
\draw [brace] (9.5,2)  -- (10.5,2) node[yshift=0.5cm, xshift=-0.5cm] {Output};

\draw(1,1) circle(0.5cm) node {$\boldmath{x}$};
\draw(4,1)[fill=gray!30]circle(0.5cm) node {$\boldmath{h}^{(1)}$};
\draw(7,1)[fill=gray!30] circle(0.5cm) node {$\boldmath{h}^{(2)}$};
\draw(10,1) circle(0.5cm) node {$\hat{y}$};

\draw[-&gt;, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](1,1) -- (4,1);
\draw[-&gt;, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](4,1) -- (7,1);
\draw[-&gt;, shorten &gt;= 0.55cm, shorten &lt;= 0.5cm](7,1) -- (10,1);

:libs: arrows,calc,positioning,shadows.blur,decorations.pathreplacing,arrows.meta,bm" /></p>
</div><p class="caption"><strong>Figure 2:</strong> The same three layer network as in Figure 1, represented
in a shorthand form where to units of each layer are collapsed onto
one circle.</p>
<p>Let’s “zoom in” on one of the layers to see what is happening under the hood
when our neural network is running in feedforward mode. The layer
<img class="math" src="_images/math/50d2fd5973156f7213ed886ae1acd607ef36fbbb.svg" alt="f^{(i)}(x)"/> performs the computation</p>
<div class="math">
<p><img src="_images/math/4e81fe183e6dc3573563a35e0283495aa01cec08.svg" alt="\bm{a}^{(i)} = \phi(\bm{W}^{(i)^T} \bm{x} + \bm{b}^{(i)})."/></p>
</div><p>In this diagram, <img class="math" src="_images/math/47802046a50e7bd660362b55abedfc811fc20f68.svg" alt="\bm{a}^{(i)}"/> is the activated output, <img class="math" src="_images/math/50e4ea8ea3d41ae138489d95282106965cf95e9a.svg" alt="\phi"/>
represents the activation function, <img class="math" src="_images/math/a0e4f6e425d8ba95a8ad6b977225c9741f2c79ad.svg" alt="\bm{W}^{(i)^T}"/> represents a learned
matrix of weights at this layer, <img class="math" src="_images/math/5e2c94cce7d4fa1c61e2b8d2040ac6c8e1375437.svg" alt="\bm{b}^{(i)}"/> represents a learned
vector of bias terms at this layer, and <img class="math" src="_images/math/9cb903938e2965877b3c1e0513f2545947e6ed25.svg" alt="\bm{x}"/> represents the input at
this layer.</p>
<p>Neural networks rely on a nonlinear activation function to learn nonlinear
relationships. Without a nonlinear activation function, a neural network is
nothing more than a linear model. There are several choices one can make for
activation functions, including but not limited to tanh, sigmoid, and the
rectified linear unit, or ReLU for short.</p>
<p>Upon completion of the feedforward operation, the prediction <img class="math" src="_images/math/b2e9337d004c9936146ec54ec2adb06cbccaa371.svg" alt="\hat{y}"/>
is ouput from the final layer.</p>
<p>Slugnet represents a neural network as a <code class="code docutils literal"><span class="pre">Model</span></code>. You can run a
neural network in feedforward mode by calling <code class="code docutils literal"><span class="pre">model.transform(X)</span></code>
on a model, where <code class="code docutils literal"><span class="pre">X</span></code> is a matrix of inputs. In this case <code class="code docutils literal"><span class="pre">X</span></code>
is a matrix to allow users of Slugnet to make several predictions
in one call to <code class="code docutils literal"><span class="pre">model.transform</span></code>. Before you can run a model
in feedforward mode, it must be trained. This leads us to backpropogation and
optimization.</p>
</div>
<div class="section" id="backpropogation-and-optimization">
<h2>Backpropogation and Optimization<a class="headerlink" href="#backpropogation-and-optimization" title="Permalink to this headline">¶</a></h2>
<p>Training a neural network is similar to training traditional discrininative
models such as logistic regression. For instamce, we need a loss function, we
must compute derivatives, and we must implement some numerical algorithm to
optimize the model. On the other hand, neural networks are somewhat unique in
that they require us to compute a gradient at each layer with which we may
learn weights. To compute this graident, we use the backpropogation algorithm.</p>
<p>Before we can run backpropogation, a version of the feedforward algorithm
described earlier must be run, only instead of throwing away the intermedite
outputs at each layer, we store them, knowing that we’ll need them later
for backpropogation. Additionally, during training, we require the ground truth
labels or values of each sample. That is, the dataset <img class="math" src="_images/math/29ad5318b3172b2c8e972f82e660851caaa82604.svg" alt="\mathcal{D}"/>
consists of <img class="math" src="_images/math/e1773ea381d3987ec9613e3818347fa7aa0f4538.svg" alt="\{\bm{x}_n, \bm{y}_n\}_{n=1}^N"/>, where <img class="math" src="_images/math/4a6eff0e4345cd8624454f268afd65f3a16bae7b.svg" alt="N"/> is the
number of samples, and <img class="math" src="_images/math/7af6d1f5cbfcec7adb47dd68dd6d2a5ffd24e352.svg" alt="\bm{y}_n"/> is the ground truth label or output
value for sample <img class="math" src="_images/math/990111a4bffdbd510dcb5eb173596c710d263d58.svg" alt="\bm{x}_n"/>.</p>
<div class="section" id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p>Upon completion of the forward pass on a batch of inputs, we can compute the
loss for the batch using the predicted outputs, <img class="math" src="_images/math/54262162b9b81471ce8b512e5296071cfa65febf.svg" alt="\hat{\bm{y}}"/>, and
the ground truth labels or values <img class="math" src="_images/math/f0577c9959f48ae42e6672975292fa403559f0dd.svg" alt="\bm{y}"/>. Loss functions are
occasionally referred to as objective functions.</p>
<div class="math">
<p><img src="_images/math/2262caea9297c0eecc32462ec2bc60769164cede.svg" alt="\bm{\ell} = -\frac{1}{N}
   \sum_{i=1}^N \big[
      \bm{y}_i \log(\hat{\bm{y}}_i) + (1 - \bm{y}_i) \log(1 - \hat{\bm{y}}_i)
   \big]"/></p>
</div><p class="caption"><strong>Equation 1:</strong> Binary cross entropy loss function.</p>
<p>If the outputs that we are learning are binary labels, then we might use
a binary cross entropy loss function, seen in equation 1. On the other hand, if
we are learning labels with multiple classes, we might use categorical cross
entropy. The resulting loss value will inform us about how our network
performed on the batch it just predicted. We can use this value along with
validation to determine if our model is overfitting or underfitting the data.</p>
</div>
<div class="section" id="backpropogation">
<h3>Backpropogation<a class="headerlink" href="#backpropogation" title="Permalink to this headline">¶</a></h3>
<p>Backpropogation involves computing gradients for the weights <img class="math" src="_images/math/4db22cfb85bacc78cb98d8717853c5828cf6e91b.svg" alt="\bm{W}^{(i)}"/>
and bias <img class="math" src="_images/math/5e2c94cce7d4fa1c61e2b8d2040ac6c8e1375437.svg" alt="\bm{b}^{(i)}"/> for all layers <img class="math" src="_images/math/2a7a413c7e151c3da66abb41f0c6b30b9e8748bb.svg" alt="i \in \{1, 2, \dots, l\}"/>
where <img class="math" src="_images/math/a34aeb2a80a217bc4bdec69d13d207de3b573242.svg" alt="l"/> is the number of layers in our network. Once we’ve computed
these gradients, the model can use a numerical optimization method to adjust
weights and bias terms in such a way that error is reduced. Before defining the
gradients of our weights and bias terms, we must compute the activated gradient
of the current layer <img class="math" src="_images/math/86e98ad70727f445350b1850d18ebbf8140fe049.svg" alt="i"/> using the gradient computed by the previous
layer <img class="math" src="_images/math/6e047693ec0e05a5c3fd5d6af348953498a65b54.svg" alt="i + 1"/>.</p>
<div class="math">
<p><img src="_images/math/4f86199b6345e5415b2415cbc62a82c4b4c22fa6.svg" alt="\bm{g}_{\text{activated}}^{(i)} = \bm{g}_a^{(i)} = \bm{g}^{(i)} \circ \phi'(\bm{a}^{(i)})"/></p>
</div><p class="caption"><strong>Equation 2:</strong> The definition of our activation gradient at layer <img class="math" src="_images/math/86e98ad70727f445350b1850d18ebbf8140fe049.svg" alt="i"/>.
The variable <img class="math" src="_images/math/47802046a50e7bd660362b55abedfc811fc20f68.svg" alt="\bm{a}^{(i)}"/> reprsenets the activated output at layer
<img class="math" src="_images/math/86e98ad70727f445350b1850d18ebbf8140fe049.svg" alt="i"/> and <img class="math" src="_images/math/caf441de348c2832ed4004334016e1b908c488a5.svg" alt="\phi'"/> represents the derivative of the activation
function.</p>
<p>Now, we have all we need to define the gradients of our weights and bias term.</p>
<div class="math">
<p><img src="_images/math/bb46cc77a8f4687ff1126034b75a420096e62bd1.svg" alt="\nabla_{\bm{W}^{(i)}}L &amp;= \bm{g}_a^{(i)}\, \bm{h}^{(i-1)^T} \\
\nabla_{\bm{b}^{(i)}}L &amp;= \bm{g}_a^{(i)}"/></p>
</div><p class="caption"><strong>Equation 3:</strong> This equation defines the gradients of weight and bias terms,
<img class="math" src="_images/math/a0e4f6e425d8ba95a8ad6b977225c9741f2c79ad.svg" alt="\bm{W}^{(i)^T}"/> and <img class="math" src="_images/math/5e2c94cce7d4fa1c61e2b8d2040ac6c8e1375437.svg" alt="\bm{b}^{(i)}"/>. In this equation,
<img class="math" src="_images/math/05e476d0c82dae43b8eade2376befd60d9f0f8fc.svg" alt="\bm{h}^{i-1}"/> is the ouput from layer <img class="math" src="_images/math/79821eabe184f50bf3d5a0890388760a2083c442.svg" alt="i - 1"/>.</p>
<p>The only part of the computation that is missing is that of
<img class="math" src="_images/math/831059f89f5fa65bfd18f06d503c3adefb6b1d24.svg" alt="\bm{g}^{(i+1)}"/> for the next layer in the backpropogation algorithm.
This is definted in equation 3, and we can now see a recursive method of
computing gradients from layer to layer.</p>
<div class="math">
<p><img src="_images/math/c411b098f7a69c61b0e1c5b5a7ad1bb1b0039fa3.svg" alt="\bm{g}^{(i-1)} = \bm{W}^{(i)^T} \bm{g}_a^{(i)}"/></p>
</div><p class="caption"><strong>Equation 4:</strong> This defines how to propogate the gradient from
layer <img class="math" src="_images/math/86e98ad70727f445350b1850d18ebbf8140fe049.svg" alt="i"/> to layer <img class="math" src="_images/math/79821eabe184f50bf3d5a0890388760a2083c442.svg" alt="i - 1"/>.</p>
</div>
</div>
<div class="section" id="api-documentation">
<h2>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slugnet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#feedforward-mode">Feedforward Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backpropogation-and-optimization">Backpropogation and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backpropogation">Backpropogation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-documentation">API Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html">Loss Function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h3><a href="#">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slugnet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#feedforward-mode">Feedforward Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backpropogation-and-optimization">Backpropogation and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backpropogation">Backpropogation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-documentation">API Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html">Loss Function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Function</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
      <li>Next: <a href="layers.html" title="next chapter">Layers</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Jarrod Kahn.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>