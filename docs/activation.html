
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Activation Functions &#8212; Slugnet 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/caption.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Loss Functions" href="loss.html" />
    <link rel="prev" title="Layers" href="layers.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="activation-functions">
<h1>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h1>
<p>Neural networks rely on a nonlinear transformation to learn
nonlinear relationships in data. These nonlinear transformations
are typically fixed functions that are applied after a linear transformation
of the data. The linear transformation uses learned weights, while the
nonlinear function is fixed in that there are no learned parameters. In
most cases, these nonlinear functions can be thought of as activation
functions that indicate the state of a unit within a layer of a neural
network, given some data.</p>
<span class="target" id="module-slugnet.activation"></span><dl class="class">
<dt id="slugnet.activation.ReLU">
<em class="property">class </em><code class="descclassname">slugnet.activation.</code><code class="descname">ReLU</code><a class="reference internal" href="_modules/slugnet/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.activation.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.activation.Activation</span></code></p>
<p>The common rectified linean unit, or ReLU activation funtion.</p>
<p>A rectified linear unit implements the nonlinear function
<img class="math" src="_images/math/659abaf5da16b0154f213960cca680dc81a70f00.svg" alt="\phi(z) = \text{max}\{0, z\}"/>.</p>
<p>(<a class="reference external" href=".//activation-1.py">Source code</a>, <a class="reference external" href=".//activation-1.png">png</a>, <a class="reference external" href=".//activation-1.hires.png">hires.png</a>, <a class="reference external" href=".//activation-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/activation-1.png" src="_images/activation-1.png" />
</div>
</dd></dl>

<dl class="class">
<dt id="slugnet.activation.Tanh">
<em class="property">class </em><code class="descclassname">slugnet.activation.</code><code class="descname">Tanh</code><a class="reference internal" href="_modules/slugnet/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.activation.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.activation.Activation</span></code></p>
<p>The hyperbolic tangent activation function.</p>
<p>A hyperbolic tangent activation function implements the
nonlinearity given by <img class="math" src="_images/math/a683538029f3dc2a307775939afb01f214848f8c.svg" alt="\phi(z) = \text{tanh}(z)"/>, which is
equivalent to <img class="math" src="_images/math/6b60aa9c2ca5909999f91523493c21fef2e5204c.svg" alt="\sfrac{\text{sinh}(z)}{\text{cosh}(z)}"/>.</p>
<p>(<a class="reference external" href=".//activation-2.py">Source code</a>, <a class="reference external" href=".//activation-2.png">png</a>, <a class="reference external" href=".//activation-2.hires.png">hires.png</a>, <a class="reference external" href=".//activation-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/activation-2.png" src="_images/activation-2.png" />
</div>
</dd></dl>

<dl class="class">
<dt id="slugnet.activation.Sigmoid">
<em class="property">class </em><code class="descclassname">slugnet.activation.</code><code class="descname">Sigmoid</code><a class="reference internal" href="_modules/slugnet/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.activation.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.activation.Activation</span></code></p>
<p>Represent a probability distribution over two classes.</p>
<p>The sigmoid function is given by <img class="math" src="_images/math/1be2523352039f26ae354e99dd3e12a703091fd5.svg" alt="\phi(z) = \frac{1}{1 + e^{-z}}"/>.</p>
<p>(<a class="reference external" href=".//activation-3.py">Source code</a>, <a class="reference external" href=".//activation-3.png">png</a>, <a class="reference external" href=".//activation-3.hires.png">hires.png</a>, <a class="reference external" href=".//activation-3.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/activation-3.png" src="_images/activation-3.png" />
</div>
</dd></dl>

<dl class="class">
<dt id="slugnet.activation.Softmax">
<em class="property">class </em><code class="descclassname">slugnet.activation.</code><code class="descname">Softmax</code><a class="reference internal" href="_modules/slugnet/activation.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.activation.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.activation.Activation</span></code></p>
<p>Represent a probability distribution over <img class="math" src="_images/math/32f0a59be9113850c78c51bd8bbf03c53f3d4e10.svg" alt="n"/> classes.</p>
<p>The softmax activation function is given by</p>
<div class="math">
<p><img src="_images/math/d3c0ebc78ad44c05bef6829f3a887da66603c510.svg" alt="\phi(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \,
\forall \, i \in \{1, \dots, K\}"/></p>
</div><p>where <img class="math" src="_images/math/1f70281bd593efa5ac027bdd28aa795c90b8f501.svg" alt="K"/> is the number of classes. We can see that softmax is
a generalization of the sigmoid function to <img class="math" src="_images/math/32f0a59be9113850c78c51bd8bbf03c53f3d4e10.svg" alt="n"/> classes. Below,
we derive the sigmoid function using softmax with two classes.</p>
<div class="math">
<p><img src="_images/math/00ec6a31a68630434a8c636291b574a9c3fcdf13.svg" alt="\phi(z_1) &amp;= \frac{e^{z_1}}{\sum_{i=1}^2 e^{z_i}} \\
          &amp;= \frac{1}{e^{z_1 - z_1} + e^{z_2 - z_1}} \\
          &amp;= \frac{1}{1 + e^{-z_1}}, \, \text{substituting} \, z_2 = 0"/></p>
</div><p>We substitute <img class="math" src="_images/math/89d95c4120b1dce2f40bde9489c0db93e7a4bb42.svg" alt="z_2 = 0"/> because we only need one variable to
represent the probability distribution over two classes. This leaves
us with the definition of the sigmoid function.</p>
</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Introduction to Neural Nets</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html#table-of-contents">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#feedforward-mode">Feedforward Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#loss-backpropogation-and-optimization">Loss, Backpropogation, and Optimization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Layers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizers.html">Optimizers</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="layers.html" title="previous chapter">Layers</a></li>
      <li>Next: <a href="loss.html" title="next chapter">Loss Functions</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/activation.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Jarrod Kahn.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/activation.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/kahnvex/slugnet" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>