
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Layers &#8212; Slugnet 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/caption.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Activation Functions" href="activation.html" />
    <link rel="prev" title="Models" href="model.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="module-slugnet.layers">
<span id="layers"></span><h1>Layers<a class="headerlink" href="#module-slugnet.layers" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="slugnet.layers.Dense">
<em class="property">class </em><code class="descclassname">slugnet.layers.</code><code class="descname">Dense</code><span class="sig-paren">(</span><em>ind</em>, <em>outd</em>, <em>activation=&lt;slugnet.activation.Noop object&gt;</em>, <em>init=&lt;slugnet.initializations.GlorotUniform object&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slugnet/layers.html#Dense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.layers.Dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.layers.Layer</span></code></p>
<p>A common densely connected neural network layer.</p>
<p>The <code class="code docutils literal"><span class="pre">Dense</span></code> layer implements the feed forward operation</p>
<div class="math">
<p><img src="_images/math/7cafe553d169f705c18559a9e09fbfa481a015e5.svg" alt="\[
    \bm{a} = \phi(\bm{W}^T \bm{x} + \bm{b})
\]"/></p>
</div><p>where <img class="math" src="_images/math/7781592845d244c06254fdaca3880786f02b3f80.svg" alt="\bm{a}"/> is activated output, <img class="math" src="_images/math/e3bf1ef0dd807d03fb49d7b38cc22fb494d4009d.svg" alt="\phi"/>
is the activation function, <img class="math" src="_images/math/86872fb5bbd58bd98cd64a475fb814ea67d8c520.svg" alt="\bm{W}"/> are weights,
<img class="math" src="_images/math/1d735dcd1d65f80245147548ce61a9d66202bb15.svg" alt="\bm{b}"/> is our bias.</p>
<p>On feed backward, or backpropogation, the <code class="code docutils literal"><span class="pre">Dense</span></code> layer
calculates two values as follows</p>
<div class="math">
<p><img src="_images/math/5c2680bc4b8caeb1698eaa50c3ca1dc90b6632e0.svg" alt="\begin{flalign}
    \frac{\partial \ell}{\partial \bm{a}^{(i)}} &amp;=
        \Big[ \bm{W}^{(i + 1)^T}
        \frac{\partial \ell}{\partial \bm{a}^{(i + 1)}}\Big]
        \circ \phi'(\bm{a}^{(i)}) \\
    \frac{\partial \ell}{\partial \bm{W}^{(i)}} &amp;=
        \frac{\partial \ell}{\partial \bm{a}^{(i)}} \bm{x}^T
\end{flalign}"/></p>
</div><p>When looking at the source, there is a notable absence of
<img class="math" src="_images/math/b9d73c3c4500c83029d653faaa1dcf3afd70e99f.svg" alt="\bm{W}^{(i + 1)^T}"/>
and <img class="math" src="_images/math/b52043529fabcd9ab1b5932757104cc9c2e58f87.svg" alt="\frac{\partial \ell}{\partial \bm{a}^{(i + 1)}}"/>.
This is because their dot product is calculated in the previous layer.
The model propogates that gradient to this layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ind</strong> (<em>int</em>) – The input dimension at this layer.</li>
<li><strong>outd</strong> (<em>int</em>) – The output dimension at this layer.</li>
<li><strong>activation</strong> (<em>slugnet.activation.Activation</em>) – The activation function to be used at the layer.</li>
<li><strong>init</strong> (<em>slugnet.initializations.Initializer</em>) – The initialization function to be used</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="slugnet.layers.Dense.backprop">
<code class="descname">backprop</code><span class="sig-paren">(</span><em>nxt_grad</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slugnet/layers.html#Dense.backprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.layers.Dense.backprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the derivative for the next layer
and computes update for this layers weights</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="slugnet.layers.Dropout">
<em class="property">class </em><code class="descclassname">slugnet.layers.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.0</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slugnet/layers.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.layers.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.layers.Layer</span></code></p>
<p>Dropout is a method of regularization that trains subnetworks by turning
off non-output nodes with some probability <img class="math" src="_images/math/70c2f41478031b532a625cb53a08693e6ea15602.svg" alt="p"/>.</p>
<p>This approximates bagging, which involves training an ensemble of models
to overcome weaknesses in any given model <a href="#id7"><span class="problematic" id="id1">[1]_</span></a>.</p>
<p>We can formalize dropout by representing the subnetworks created by dropout
with a mask vector <img class="math" src="_images/math/b5b3102a9e7573467f8dc78c71a3f2371ddc4ef0.svg" alt="\bm{\mu}"/>. Now, we note each subnetwork defines a
new probability distribution of <img class="math" src="_images/math/ddc4cefb70948005dd1900a3de93cefcc7b1b6b3.svg" alt="y"/> as
<img class="math" src="_images/math/35467d5760c0b5bc94e558d3dad144b818775027.svg" alt="\mathds{P}(y | \bm{x}, \bm{\mu})"/> <a href="#id8"><span class="problematic" id="id2">[1]_</span></a>. If we define
<img class="math" src="_images/math/9dbf7cd080c8a3c2bb81218bb66184ea28673e89.svg" alt="\mathds{P}(\bm{\mu})"/> as the probability distribution of mask vectors
<img class="math" src="_images/math/b5b3102a9e7573467f8dc78c71a3f2371ddc4ef0.svg" alt="\bm{\mu}"/>, we can write the mean of all subnetworks as</p>
<div class="math">
<p><img src="_images/math/f7d65edfc282dc4351e1169855a6ecedadf571b8.svg" alt="\[
    \sum_{\bm{\mu}} \mathds{P}(\bm{\mu}) \mathds{P}(y | \bm{x}, \bm{\mu}).
\]"/></p>
</div><p>The problem with evaluating this term is the exponential number of mask
vectors. In practice, we approximate this probability distribution by
including all nodes during inference, and multiplying each output by
<img class="math" src="_images/math/b31dbce22ac4f932167d1dab800282625ddb22b5.svg" alt="1 - p"/>, the probability that any node is included in the network during
training. This rule is called the weight scaling inference rule <a href="#id9"><span class="problematic" id="id3">[1]_</span></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>p</strong> (<em>float</em>) – The probability of a non-ouput node being removed from the network.</td>
</tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Goodfellow, Bengio, Courville (2016), Deep Learning, <a class="reference external" href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a></td></tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="slugnet.layers.Convolution">
<em class="property">class </em><code class="descclassname">slugnet.layers.</code><code class="descname">Convolution</code><span class="sig-paren">(</span><em>ind</em>, <em>nb_kernel</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>init=&lt;slugnet.initializations.GlorotUniform object&gt;</em>, <em>activation=&lt;slugnet.activation.ReLU object&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slugnet/layers.html#Convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slugnet.layers.Convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">slugnet.layers.Layer</span></code></p>
<p>A layer that implements the convolution operation.</p>
<p>In the general case, a discrete convolution operation implements
the function:</p>
<div class="math">
<p><img src="_images/math/b785a0d6cfc7114705adac1dc8a868b188f7c75e.svg" alt="\[s(i) = \sum_{a=-\infty}^\infty x(a) w(i - a)\]"/></p>
</div><p>where <img class="math" src="_images/math/0642a80e44de7b43d141fa9d602bc25a316a12ba.svg" alt="x"/> is the input and <img class="math" src="_images/math/bf3580fe7b9c3cee41223cb47ebbe60dac20b359.svg" alt="w"/>
is the kernel, or in some cases the weighting function.</p>
<p>In the case of convolutional neural networks, the input
is typically two dimensional image <img class="math" src="_images/math/82d557a1887f8018e73b98245468e9e76ac99dc5.svg" alt="I"/>, and it
follows that we have a two dimensional kernel <img class="math" src="_images/math/1f70281bd593efa5ac027bdd28aa795c90b8f501.svg" alt="K"/>.
Now we can write out convolution function with both axes:</p>
<div class="math">
<p><img src="_images/math/5fe607911c726afc17d064f98616b2ad916850b6.svg" alt="\[S(i, j) = \sum_m \sum_n I(m, n) K(i - m, j - n).\]"/></p>
</div><p>Note that we can write the infinite sum over the domains of
<img class="math" src="_images/math/1e147e70e7d7a2b61cee9f9ee7e6f24faffb9b8a.svg" alt="m"/> and <img class="math" src="_images/math/32f0a59be9113850c78c51bd8bbf03c53f3d4e10.svg" alt="n"/> as discrete sums because we assume
that the kernel <img class="math" src="_images/math/1f70281bd593efa5ac027bdd28aa795c90b8f501.svg" alt="K"/> is zero everywhere but the set of
points in which we store data <a href="#id10"><span class="problematic" id="id5">[1]_</span></a>.</p>
<p>The motivation for using the convolution operation in a
neural network is best described using an example of an
image. In a densely connected neural network, each node
at layer <img class="math" src="_images/math/6b271c0920f320c51b2c7112c10eb00471a3673d.svg" alt="i"/> is connected to every node at layer
<img class="math" src="_images/math/b8c29d6a3b774d66617715a65662a284a79c345e.svg" alt="i + 1"/>. This does not lend itself to image processing,
where location of a shape relative to another shape is
important. For instance, finding a right angle involves
detecting two edges that are perpendicular, <em>and</em> whose
lines cross one another. If we make the kernel smaller
than the input image, we can process parts of the image
at a time, thereby ensuring locality of the input signals.
To process the entire image, we slide the kernel over the
input, along both axes. At each step, an output is produced
which will be used as input for the next layer.
This configuration allows us to learn the parameters of the
kernel <img class="math" src="_images/math/1f70281bd593efa5ac027bdd28aa795c90b8f501.svg" alt="K"/> the same way we’d learn ordinary parameters
in a densely connected neural network.</p>
<div class="figure">
<p><img src="_images/tikz-c88077e4f163128088cfeaf5472bf1c09ad70fa9.png" alt="\def\input {
    0/2.4/a,
    1.2/2.4/b,
    2.4/2.4/c,
    0/1.2/d,
    1.2/1.2/e,
    2.4/1.2/f,
    0/0/g,
    1.2/0/h,
    2.4/0/i
}

\def\kernel {
    0/1.2/w,
    1.2/1.2/x,
    0/0/y,
    1.2/0/z
}

\def\output {
    0/-4.6/aw + bx + dy + ez,
    3.4/-4.6/bw + cx + ey + fz,
    0/-8/dw + ex + gy + hz,
    3.4/-8/ew + fx + hy + iz
}

\draw (0.5,3.8) node {Input};
\foreach \x/\y/\l in \input
    \draw (\x,\y) -- (\x,\y + 1) -- (\x + 1,\y + 1) -- (\x + 1,\y) -- (\x,\y)
    node[anchor=south west]{$\l$};

\draw (5.5,2.6) node {Kernel};
\foreach \x/\y/\l in \kernel
    \draw (\x + 5,\y) -- (\x + 5, \y + 1) -- (\x + 6, \y + 1) -- (\x + 6, \y) -- (\x + 5, \y)
    node[anchor=south west]{$\l$};

\draw (0.7,-1.3) node {Output};
\foreach \x/\y/\l in \output
    \draw (\x,\y) -- (\x,\y + 3) -- (\x + 3,\y + 3) -- (\x + 3, \y) -- (\x,\y)
    node[xshift=1.5cm, yshift=1.5cm]{\footnotesize $\l$};

\draw (1.1,3.5) -- (3.5, 3.5) -- (3.5, 1.1) -- (1.1, 1.1) -- (1.1, 3.5);
\draw (4.9,2.3) -- (7.3, 2.3) -- (7.3, -0.1) -- (4.9, -0.1) -- (4.9, 2.3);

\draw [-|&gt;] (3.5, 2.3) -- (4.0, 2.3) -- (4.0, -1.5);
\draw [-|&gt;] (6, -0.1) -- (6, -1.5);" /></p>
</div><p class="caption"><strong>Figure 1:</strong> An example of a two dimension convolution operation. The
input is an image in <img class="math" src="_images/math/23072a119b7a7c7cf2d6c947df6281370e5517f8.svg" alt="\mathds{R}^{3 \times 3}"/>, and the kernel is
in <img class="math" src="_images/math/a88cdf031b73d22f069872b57c8021b71537a760.svg" alt="\mathds{R}^{2 \times 2}"/>. As the kernel is slid over the input
with a stride width of one, an output in
<img class="math" src="_images/math/a88cdf031b73d22f069872b57c8021b71537a760.svg" alt="\mathds{R}^{2 \times 2}"/> is produced. In the example, the arrows
and boxes demonstrate how the upper-right portion of the input image
are compbined with the kernel parameters to produce the upper right
unit of output.</p>
<p class="caption">–Source: Goodfellow, Bengio, Courville (Deep Learning, 2016, Figure 9.1).</p>
<p>The stride width determinse how far the kernel moves at each step. Of
course, to learn anything interesting, we require multiple kernels at
each layer. These are all configurable hyperparameters that can be set
upon network instantiation. When the network is operating in feedforward
mode, the output at each layer is a three dimensional tensor, rather than
a matrix. This is due to the fact that each kernel produces its own
two dimensional output, and there are multiple kernels at every layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nb_kernel</strong> (<em>int</em>) – The number of kernels to use.</li>
<li><strong>kernel_size</strong> (<em>(</em><em>int</em><em>, </em><em>int</em><em>)</em>) – The size of the kernel as a tuple, heigh by width</li>
<li><strong>stride</strong> (<em>int</em>) – The stide width to use</li>
<li><strong>init</strong> (<em>slugnet.initializations.Initializer</em>) – The initializer to use</li>
<li><strong>activation</strong> (<em>slugnet.activation.Activation</em>) – The activation function to be used at the layer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Goodfellow, Bengio, Courville (2016), Deep Learning, Chapter 9,
<a class="reference external" href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a></td></tr>
</tbody>
</table>
</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Introduction to Neural Nets</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html#table-of-contents">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#feedforward-mode">Feedforward Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#loss-backpropogation-and-optimization">Loss, Backpropogation, and Optimization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="activation.html">Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizers.html">Optimizers</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="model.html" title="previous chapter">Models</a></li>
      <li>Next: <a href="activation.html" title="next chapter">Activation Functions</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/layers.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, <a href="https://jarrodkahn.com">Jarrod Kahn</a>.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/layers.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/kahnvex/slugnet" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>